{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will find an example of Bidirectional LSTM implemented in PyTorch.\n",
    "\n",
    "The task that we will try to solve in this notebook is MNIST images classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim import Adam\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(preds, y_true):\n",
    "    '''\n",
    "    Use this function to check accuracy of a model trained.\n",
    "    \n",
    "    :param: preds - predictions generated by neural network\n",
    "    :param: y_true - true/real labels for each sample in the dataset\n",
    "    '''\n",
    "    correct = 0 \n",
    "    assert len(preds) == len(y_true)\n",
    "    \n",
    "    for i in range(len(preds)):\n",
    "        if np.argmax(preds[i]) == y_true[i]:\n",
    "            correct += 1\n",
    "    return correct / len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparams\n",
    "#How many samples do we feed to the NN at onces\n",
    "batch_size = 100\n",
    "#step size-this param is used by optimizer\n",
    "learning_rate = 0.003 \n",
    "#How many times do we want to go through all samples in a dataset \n",
    "#NOTE: more is not always batter. More epochs can lead to overfit if you didnt regularize a network properly\n",
    "epochs = 3\n",
    "#number of RNN layers \n",
    "number_of_layers = 2\n",
    "#Number of units/neurons in LSTM cell\n",
    "rnn_units = 128\n",
    "#If bi_dir == True LSTM layer will be bidirectional\n",
    "bi_dir = True\n",
    "#Number of numbers in the input vector (MNIST images are 28x28, so we put in_size 28 and sequnce_length to 28)\n",
    "in_size = 28\n",
    "seq_len = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset preparation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Download and transform MNIST training set\n",
    "train_dataset = MNIST(root='./data/', \n",
    "                      train=True, \n",
    "                      transform=transforms.ToTensor(), \n",
    "                      download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Unpack and transform MNIST test set\n",
    "test_dataset = MNIST(root='./data/', \n",
    "                      train=False, \n",
    "                      transform=transforms.ToTensor(), \n",
    "                      download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create training set loader - DataLoader will help us with batching a dataset\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True, \n",
    "                                           num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create test set loader\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False, \n",
    "                                           num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Bidirectional RNN in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BiRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_size, seq_len, rnn_units, number_of_layers, num_classes):\n",
    "        '''\n",
    "            Init function of the BiRNN class helps to setup everything that we need for bidirectional rnn.\n",
    "            \n",
    "            :param: in_size - size of an input vector\n",
    "            :param: seq_len - how many time steps a network will have\n",
    "            :param: rnn_units - number of units/neurons in the RNN\n",
    "            :param: number_of_layers - number of RNN layers\n",
    "            :param: num_classes - number of different classes in a dataset (e.p. MNIST has 10 classes)\n",
    "        '''\n",
    "        super(BiRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_units = rnn_units\n",
    "        self.num_layers = number_of_layers\n",
    "        \n",
    "        self.rnn = nn.LSTM(in_size, rnn_units, number_of_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        #Put number of units x2 -> We are doing this because of Bidirectional RNN\n",
    "        self.output = nn.Linear(self.hidden_units*2, num_classes)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        '''\n",
    "        This method is called for networks forward-prop.\n",
    "        \n",
    "        :param: X - batch of data from a dataset\n",
    "        '''\n",
    "        \n",
    "        #Create starting states for the LSTM layer\n",
    "        h0 = Variable(torch.zeros(self.num_layers*2, X.size(0), self.hidden_units)).cuda()\n",
    "        #NOTE: that we have number of layers * 2 -> that is because we are using Bidirectional RNN\n",
    "        c0 = Variable(torch.zeros(self.num_layers*2, X.size(0), self.hidden_units)).cuda()\n",
    "        \n",
    "        out, _ = self.rnn(X, (h0, c0))\n",
    "        return f.softmax(self.output(out[:, -1, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create BiRNN network object\n",
    "rnn = BiRNN(in_size, seq_len, rnn_units, number_of_layers, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiRNN(\n",
       "  (rnn): LSTM(28, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (output): Linear(in_features=256, out_features=10)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#If you have avaliable cuda supported GPU card use this command to run the network on GPU instead of CPU\n",
    "rnn.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create object of the cross entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create object of Adam optimizer\n",
    "optimizer = Adam(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lukaa\\appdata\\local\\continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3  | Epoch loss: 1.7570611238479614  | Epoch accuracy: 0.7049\n",
      "Epoch: 2/3  | Epoch loss: 1.5680830478668213  | Epoch accuracy: 0.8934999999999998\n",
      "Epoch: 3/3  | Epoch loss: 1.5165644884109497  | Epoch accuracy: 0.9450999999999999\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch_accuracy = [] \n",
    "    epoch_loss = []\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        \n",
    "        #Create batch of images and labels\n",
    "        X_batch = Variable(images.view(-1, seq_len, in_size)).cuda()\n",
    "        y_batch = Variable(labels).cuda()\n",
    "        \n",
    "        optimizer.zero_grad() #set network grads to zero\n",
    "        preds = rnn(X_batch) #get network prediction\n",
    "        epoch_accuracy.append(accuracy(preds.cpu().data.numpy(), y_batch.cpu().data.numpy())) #get accuracy for the current batch\n",
    "        loss = criterion(preds, y_batch) #calculate cross entropy loss for a given batch\n",
    "        epoch_loss.append(loss.cpu().data.numpy()) #log batch loss\n",
    "        loss.backward() #call backprop function in respect to calculated loss\n",
    "        optimizer.step() #finally we call optimizers step function\n",
    "        \n",
    "    print(\"Epoch: {}/{}\".format(epoch+1, epochs), \n",
    "          \" | Epoch loss: {}\".format(np.mean(epoch_loss)), \n",
    "          \" | Epoch accuracy: {}\".format(np.mean(epoch_accuracy)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
