{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets \n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparams\n",
    "batch_size = 128\n",
    "learning_rate = 0.0003\n",
    "epochs =  200\n",
    "latent_space_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dataset transform and load MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.5, 0.5, 0.5), \n",
    "                                     std=(0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = datasets.MNIST(root='./data/',\n",
    "                       train=True,\n",
    "                       transform=transform,\n",
    "                       download=True)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Discriminator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(784, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = F.leaky_relu(self.l1(X))\n",
    "        out = F.leaky_relu(self.l2(out))\n",
    "        out = F.sigmoid(self.l3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Generator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(64, 256)\n",
    "        self.l2 = nn.Linear(256, 256)\n",
    "        self.l3 = nn.Linear(256, 784)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = F.leaky_relu(self.l1(X))\n",
    "        out = F.leaky_relu(self.l2(out))\n",
    "        out = F.tanh(self.l3(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (l1): Linear(in_features=784, out_features=256)\n",
       "  (l2): Linear(in_features=256, out_features=256)\n",
       "  (l3): Linear(in_features=256, out_features=1)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (l1): Linear(in_features=64, out_features=256)\n",
       "  (l2): Linear(in_features=256, out_features=256)\n",
       "  (l3): Linear(in_features=256, out_features=784)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define optimizers for generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=learning_rate)\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lukaa\\appdata\\local\\continuum\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py:1168: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G loss: 1.8818649053573608  | D loss: 0.7400017380714417\n",
      "G loss: 1.9479326009750366  | D loss: 0.6165869832038879\n",
      "G loss: 2.1690335273742676  | D loss: 0.6458476781845093\n",
      "G loss: 2.345785617828369  | D loss: 0.5358943939208984\n",
      "G loss: 2.250821352005005  | D loss: 0.6544641256332397\n",
      "G loss: 2.5245144367218018  | D loss: 0.7299686670303345\n",
      "G loss: 2.5776891708374023  | D loss: 0.7506179809570312\n",
      "G loss: 2.0646724700927734  | D loss: 0.864018976688385\n",
      "G loss: 2.286578893661499  | D loss: 0.6973211765289307\n",
      "G loss: 2.89401912689209  | D loss: 0.7400039434432983\n",
      "G loss: 2.9077513217926025  | D loss: 0.6944419741630554\n",
      "G loss: 2.5862698554992676  | D loss: 0.6557679176330566\n",
      "G loss: 2.6391549110412598  | D loss: 0.4914310574531555\n",
      "G loss: 2.6979963779449463  | D loss: 0.5106393098831177\n",
      "G loss: 2.719245672225952  | D loss: 0.37330761551856995\n",
      "G loss: 3.0988714694976807  | D loss: 0.4231935143470764\n",
      "G loss: 3.421299695968628  | D loss: 0.4121059775352478\n",
      "G loss: 2.8859806060791016  | D loss: 0.4665166139602661\n",
      "G loss: 3.2711234092712402  | D loss: 0.43672165274620056\n",
      "G loss: 3.4169275760650635  | D loss: 0.44201454520225525\n",
      "G loss: 3.1381630897521973  | D loss: 0.4646838903427124\n",
      "G loss: 3.068988561630249  | D loss: 0.4436619281768799\n",
      "G loss: 3.1614766120910645  | D loss: 0.4275195896625519\n",
      "G loss: 2.9151313304901123  | D loss: 0.46368157863616943\n",
      "G loss: 3.050158739089966  | D loss: 0.45647555589675903\n",
      "G loss: 3.102573871612549  | D loss: 0.5184205174446106\n",
      "G loss: 2.9508609771728516  | D loss: 0.5040168166160583\n",
      "G loss: 2.6983344554901123  | D loss: 0.5602655410766602\n",
      "G loss: 2.697006940841675  | D loss: 0.5462789535522461\n",
      "G loss: 2.6193978786468506  | D loss: 0.5331448316574097\n",
      "G loss: 2.662679672241211  | D loss: 0.5524643063545227\n",
      "G loss: 2.6560089588165283  | D loss: 0.5328847169876099\n",
      "G loss: 2.6216483116149902  | D loss: 0.5488457083702087\n",
      "G loss: 2.5179505348205566  | D loss: 0.5836297273635864\n",
      "G loss: 2.6422970294952393  | D loss: 0.5890307426452637\n",
      "G loss: 2.5365655422210693  | D loss: 0.5754246115684509\n",
      "G loss: 2.5341150760650635  | D loss: 0.5800890326499939\n",
      "G loss: 2.4341132640838623  | D loss: 0.5722866058349609\n",
      "G loss: 2.467881917953491  | D loss: 0.5437994599342346\n",
      "G loss: 2.516160011291504  | D loss: 0.5378697514533997\n",
      "G loss: 2.534632682800293  | D loss: 0.5608363747596741\n",
      "G loss: 2.4701473712921143  | D loss: 0.5618617534637451\n",
      "G loss: 2.4477736949920654  | D loss: 0.5562289357185364\n",
      "G loss: 2.4980437755584717  | D loss: 0.5689746737480164\n",
      "G loss: 2.528411626815796  | D loss: 0.5432690382003784\n",
      "G loss: 2.4940216541290283  | D loss: 0.5549367070198059\n",
      "G loss: 2.353928565979004  | D loss: 0.5754673480987549\n",
      "G loss: 2.4128923416137695  | D loss: 0.5706068277359009\n",
      "G loss: 2.3013672828674316  | D loss: 0.616112232208252\n",
      "G loss: 2.3180994987487793  | D loss: 0.6074970364570618\n",
      "G loss: 2.2836251258850098  | D loss: 0.6108068227767944\n",
      "G loss: 2.3648488521575928  | D loss: 0.5896044969558716\n",
      "G loss: 2.281585216522217  | D loss: 0.5885814428329468\n",
      "G loss: 2.2686007022857666  | D loss: 0.6180375814437866\n",
      "G loss: 2.2362430095672607  | D loss: 0.6236921548843384\n",
      "G loss: 2.235684394836426  | D loss: 0.6273602247238159\n",
      "G loss: 2.2208364009857178  | D loss: 0.6393172144889832\n",
      "G loss: 2.1276068687438965  | D loss: 0.6395689845085144\n",
      "G loss: 2.1824803352355957  | D loss: 0.6434131860733032\n",
      "G loss: 2.134211778640747  | D loss: 0.640771746635437\n",
      "G loss: 2.174480438232422  | D loss: 0.6441627144813538\n",
      "G loss: 2.2292275428771973  | D loss: 0.6381360292434692\n",
      "G loss: 2.213244676589966  | D loss: 0.6430054306983948\n",
      "G loss: 2.156064987182617  | D loss: 0.6506943702697754\n",
      "G loss: 2.1319520473480225  | D loss: 0.6493212580680847\n",
      "G loss: 2.1250288486480713  | D loss: 0.659072756767273\n",
      "G loss: 2.1412270069122314  | D loss: 0.6413983106613159\n",
      "G loss: 2.1183204650878906  | D loss: 0.6490420699119568\n",
      "G loss: 2.083529472351074  | D loss: 0.6677725911140442\n",
      "G loss: 2.142282485961914  | D loss: 0.6503879427909851\n",
      "G loss: 2.0868685245513916  | D loss: 0.6804190278053284\n",
      "G loss: 2.051431179046631  | D loss: 0.6661669015884399\n",
      "G loss: 2.073509454727173  | D loss: 0.6734120845794678\n",
      "G loss: 2.084714412689209  | D loss: 0.6764719486236572\n",
      "G loss: 2.036994695663452  | D loss: 0.6946635246276855\n",
      "G loss: 1.9817017316818237  | D loss: 0.701907753944397\n",
      "G loss: 2.007127046585083  | D loss: 0.6992021799087524\n",
      "G loss: 2.0076324939727783  | D loss: 0.6993281841278076\n",
      "G loss: 1.9938057661056519  | D loss: 0.7044125199317932\n",
      "G loss: 1.9811745882034302  | D loss: 0.704931914806366\n",
      "G loss: 1.9483129978179932  | D loss: 0.7169651389122009\n",
      "G loss: 1.9646445512771606  | D loss: 0.7214215993881226\n",
      "G loss: 1.923673152923584  | D loss: 0.724051296710968\n",
      "G loss: 1.963481068611145  | D loss: 0.724885106086731\n",
      "G loss: 1.9118785858154297  | D loss: 0.7332347631454468\n",
      "G loss: 1.9668034315109253  | D loss: 0.7305473685264587\n",
      "G loss: 1.8667118549346924  | D loss: 0.7393852472305298\n",
      "G loss: 1.87698233127594  | D loss: 0.7344786524772644\n",
      "G loss: 1.8888179063796997  | D loss: 0.7400774955749512\n",
      "G loss: 1.8957003355026245  | D loss: 0.7349043488502502\n",
      "G loss: 1.8674052953720093  | D loss: 0.7465229630470276\n",
      "G loss: 1.8255548477172852  | D loss: 0.7463982701301575\n",
      "G loss: 1.8398162126541138  | D loss: 0.7494227886199951\n",
      "G loss: 1.8700449466705322  | D loss: 0.7425768375396729\n",
      "G loss: 1.8469438552856445  | D loss: 0.751154899597168\n",
      "G loss: 1.8360600471496582  | D loss: 0.7597647309303284\n",
      "G loss: 1.8386726379394531  | D loss: 0.752019464969635\n",
      "G loss: 1.8033111095428467  | D loss: 0.7620669603347778\n",
      "G loss: 1.7903971672058105  | D loss: 0.7637854814529419\n",
      "G loss: 1.8000361919403076  | D loss: 0.7690314650535583\n",
      "G loss: 1.802903413772583  | D loss: 0.7615363001823425\n",
      "G loss: 1.7843230962753296  | D loss: 0.7702509164810181\n",
      "G loss: 1.778975486755371  | D loss: 0.7704835534095764\n",
      "G loss: 1.779085636138916  | D loss: 0.7726162672042847\n",
      "G loss: 1.8071259260177612  | D loss: 0.769251823425293\n",
      "G loss: 1.7999647855758667  | D loss: 0.7729061245918274\n",
      "G loss: 1.7661110162734985  | D loss: 0.7728421688079834\n",
      "G loss: 1.74482262134552  | D loss: 0.779649019241333\n",
      "G loss: 1.7627763748168945  | D loss: 0.7859761714935303\n",
      "G loss: 1.7559781074523926  | D loss: 0.7799757122993469\n",
      "G loss: 1.7924705743789673  | D loss: 0.7768930196762085\n",
      "G loss: 1.7724882364273071  | D loss: 0.7853341102600098\n",
      "G loss: 1.735318899154663  | D loss: 0.7795487642288208\n",
      "G loss: 1.7689672708511353  | D loss: 0.7865018248558044\n",
      "G loss: 1.7470852136611938  | D loss: 0.7861717343330383\n",
      "G loss: 1.7148454189300537  | D loss: 0.7932078242301941\n",
      "G loss: 1.7482960224151611  | D loss: 0.7846872210502625\n",
      "G loss: 1.736664891242981  | D loss: 0.7986228466033936\n",
      "G loss: 1.714830756187439  | D loss: 0.7926266193389893\n",
      "G loss: 1.7062492370605469  | D loss: 0.8042683601379395\n",
      "G loss: 1.6943085193634033  | D loss: 0.797150194644928\n",
      "G loss: 1.6943646669387817  | D loss: 0.7938570976257324\n",
      "G loss: 1.7426384687423706  | D loss: 0.7915170788764954\n",
      "G loss: 1.7245793342590332  | D loss: 0.8025112152099609\n",
      "G loss: 1.704764723777771  | D loss: 0.7990249991416931\n",
      "G loss: 1.7002904415130615  | D loss: 0.806717038154602\n",
      "G loss: 1.692676067352295  | D loss: 0.8008305430412292\n",
      "G loss: 1.6957505941390991  | D loss: 0.8055247664451599\n",
      "G loss: 1.6916131973266602  | D loss: 0.8055817484855652\n",
      "G loss: 1.7031216621398926  | D loss: 0.8024820685386658\n",
      "G loss: 1.6984513998031616  | D loss: 0.8089637160301208\n",
      "G loss: 1.6998165845870972  | D loss: 0.8055076003074646\n",
      "G loss: 1.6990504264831543  | D loss: 0.8002961874008179\n",
      "G loss: 1.7043884992599487  | D loss: 0.8033896088600159\n",
      "G loss: 1.713792085647583  | D loss: 0.8007293343544006\n",
      "G loss: 1.7105923891067505  | D loss: 0.8017292618751526\n",
      "G loss: 1.7001264095306396  | D loss: 0.8025985360145569\n",
      "G loss: 1.688248872756958  | D loss: 0.8030263781547546\n",
      "G loss: 1.674002766609192  | D loss: 0.810943067073822\n",
      "G loss: 1.673439860343933  | D loss: 0.8021577000617981\n",
      "G loss: 1.7061684131622314  | D loss: 0.8061364889144897\n",
      "G loss: 1.691699504852295  | D loss: 0.7987689971923828\n",
      "G loss: 1.685228705406189  | D loss: 0.8067403435707092\n",
      "G loss: 1.7158836126327515  | D loss: 0.8043738603591919\n",
      "G loss: 1.6723006963729858  | D loss: 0.806082546710968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G loss: 1.7154077291488647  | D loss: 0.7943461537361145\n",
      "G loss: 1.6909401416778564  | D loss: 0.8006151914596558\n",
      "G loss: 1.7036904096603394  | D loss: 0.8036582469940186\n",
      "G loss: 1.7027660608291626  | D loss: 0.7974878549575806\n",
      "G loss: 1.7097532749176025  | D loss: 0.7967267632484436\n",
      "G loss: 1.6965079307556152  | D loss: 0.7972111105918884\n",
      "G loss: 1.7019339799880981  | D loss: 0.804136335849762\n",
      "G loss: 1.683194875717163  | D loss: 0.8030058741569519\n",
      "G loss: 1.7300807237625122  | D loss: 0.7951854467391968\n",
      "G loss: 1.7232540845870972  | D loss: 0.7982699275016785\n",
      "G loss: 1.7247685194015503  | D loss: 0.8010149598121643\n",
      "G loss: 1.7099969387054443  | D loss: 0.7988106608390808\n",
      "G loss: 1.698622226715088  | D loss: 0.7973784804344177\n",
      "G loss: 1.730819821357727  | D loss: 0.792112410068512\n",
      "G loss: 1.7010807991027832  | D loss: 0.793914794921875\n",
      "G loss: 1.7061855792999268  | D loss: 0.7904404997825623\n",
      "G loss: 1.712510108947754  | D loss: 0.7987266182899475\n",
      "G loss: 1.709913730621338  | D loss: 0.7971711754798889\n",
      "G loss: 1.7285836935043335  | D loss: 0.7895444631576538\n",
      "G loss: 1.7245445251464844  | D loss: 0.7919132709503174\n",
      "G loss: 1.7114206552505493  | D loss: 0.8005252480506897\n",
      "G loss: 1.7163373231887817  | D loss: 0.789482057094574\n",
      "G loss: 1.7111352682113647  | D loss: 0.7905604839324951\n",
      "G loss: 1.7181966304779053  | D loss: 0.7953827977180481\n",
      "G loss: 1.7093960046768188  | D loss: 0.7905372977256775\n",
      "G loss: 1.738232970237732  | D loss: 0.7862190008163452\n",
      "G loss: 1.729815125465393  | D loss: 0.7885880470275879\n",
      "G loss: 1.740341305732727  | D loss: 0.7900291085243225\n",
      "G loss: 1.7263833284378052  | D loss: 0.7961853742599487\n",
      "G loss: 1.7353341579437256  | D loss: 0.7834041118621826\n",
      "G loss: 1.7377420663833618  | D loss: 0.7868335247039795\n",
      "G loss: 1.7470897436141968  | D loss: 0.7840254902839661\n",
      "G loss: 1.75923752784729  | D loss: 0.7910891175270081\n",
      "G loss: 1.7413431406021118  | D loss: 0.7880959510803223\n",
      "G loss: 1.7448972463607788  | D loss: 0.7802690267562866\n",
      "G loss: 1.7351171970367432  | D loss: 0.7869364023208618\n",
      "G loss: 1.7320739030838013  | D loss: 0.7844480872154236\n",
      "G loss: 1.7268459796905518  | D loss: 0.7829504013061523\n",
      "G loss: 1.7375909090042114  | D loss: 0.7830885052680969\n",
      "G loss: 1.7580828666687012  | D loss: 0.7827555537223816\n",
      "G loss: 1.7466559410095215  | D loss: 0.7843990325927734\n",
      "G loss: 1.731809377670288  | D loss: 0.782044529914856\n",
      "G loss: 1.746199607849121  | D loss: 0.7811278104782104\n",
      "G loss: 1.7454692125320435  | D loss: 0.7764321565628052\n",
      "G loss: 1.742284893989563  | D loss: 0.7804694175720215\n",
      "G loss: 1.7498430013656616  | D loss: 0.7746408581733704\n",
      "G loss: 1.7706925868988037  | D loss: 0.7744351625442505\n",
      "G loss: 1.7593644857406616  | D loss: 0.7772218585014343\n",
      "G loss: 1.7528791427612305  | D loss: 0.7725151777267456\n",
      "G loss: 1.787467122077942  | D loss: 0.7703126072883606\n",
      "G loss: 1.7630215883255005  | D loss: 0.7751119136810303\n",
      "G loss: 1.775526762008667  | D loss: 0.7734900712966919\n",
      "G loss: 1.7622175216674805  | D loss: 0.7766178250312805\n",
      "G loss: 1.7529773712158203  | D loss: 0.7720352411270142\n",
      "G loss: 1.759612798690796  | D loss: 0.772292971611023\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    g_epoch_loss = []\n",
    "    d_epoch_loss = []\n",
    "    for images, _ in data_loader:\n",
    "        \n",
    "        real_images = Variable(images.view(batch_size, -1)).cuda()\n",
    "        fake_images = G(Variable(torch.randn(batch_size, latent_space_size).cuda()))\n",
    "        \n",
    "        if real_images.cpu().data.numpy().shape[1] == 28*28:\n",
    "            real_images_dis_outputs = D(real_images)\n",
    "            fake_images_dis_outputs = D(fake_images)\n",
    "\n",
    "            D_real_loss = criterion(real_images_dis_outputs, Variable(torch.ones(batch_size)).cuda())\n",
    "            D_fake_loss = criterion(fake_images_dis_outputs, Variable(torch.zeros(batch_size)).cuda())\n",
    "\n",
    "            #Calculate losses\n",
    "            D_loss = D_real_loss + D_fake_loss\n",
    "            G_loss = criterion(fake_images_dis_outputs, Variable(torch.ones(batch_size)).cuda())\n",
    "            \n",
    "            #Log\n",
    "            g_epoch_loss.append(G_loss.cpu().data.numpy())\n",
    "            d_epoch_loss.append(D_loss.cpu().data.numpy())\n",
    "            \n",
    "            #Update Discriminator\n",
    "            d_optimizer.zero_grad()\n",
    "            D_loss.backward(retain_graph=True)\n",
    "            d_optimizer.step()\n",
    "\n",
    "            #Update Generator\n",
    "            \n",
    "            g_optimizer.zero_grad()\n",
    "            d_optimizer.zero_grad()\n",
    "            G_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "\n",
    "    print(\"G loss: {}\".format(np.mean(g_epoch_loss)), \" | D loss: {}\".format(np.mean(d_epoch_loss))) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
